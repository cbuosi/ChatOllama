# ChatOllama 🛰️

Projeto em **C# .NET 8** — Interface para rodar LLMs da engine **Ollama** (qualquer modelo). 👀

---
![SharpWebServer](https://github.com/cbuosi/ChatOllama/blob/main/ChatOllama/wwwroot/Assets/Imagens/prev.png)

## ✨ Funcionalidades

- 📡 Interface com o **Ollama** via API (`POST`)
- 🔄 Permite escolher o modelo (**Qwen**, **DeepSeek**, **GPT** e outros)
- ⏱️ Exibe o tempo de entrada e saída dos prompts/respostas
- 📊 Mostra os totais de **Context** (histórico) e **Eval** (custo de processamento)

---

## 🚀 Como usar

1. Instale e configure o [Ollama](https://ollama.ai/) na sua máquina.  
2. Clone este repositório: 'git clone https://github.com/seuusuario/ChatOllama.git'
3. Abra o projeto no Visual Studio ou VS Code com suporte ao .NET 8.
4. Configure a URL da API do Ollama (se necessário).
5. Compile e execute o projeto.

---
⚠️ Aviso Importante 
- 👮 Nenhum bug de segurança foi identificado até o momento, porém não é recomendado usar em produção ou em servidores oficiais.
---
📌 Roadmap (Ideias Futuras)

 - Suporte a histórico persistente

 - Interface gráfica mais amigável

 - Configuração de parâmetros avançados dos modelos

 - Logs detalhados de uso

📜 Licença

- Este projeto é distribuído sob a licença GNU General Public License (GPL) 2.0
