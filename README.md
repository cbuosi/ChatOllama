# ChatOllama ğŸ›°ï¸

Projeto em **C# .NET 8** â€” Interface para rodar LLMs da engine **Ollama** (qualquer modelo). ğŸ‘€

---
![SharpWebServer](https://github.com/cbuosi/ChatOllama/blob/main/ChatOllama/wwwroot/Assets/Imagens/prev.png)

## âœ¨ Funcionalidades

- ğŸ“¡ Interface com o **Ollama** via API (`POST`)
- ğŸ”„ Permite escolher o modelo (**Qwen**, **DeepSeek**, **GPT** e outros)
- â±ï¸ Exibe o tempo de entrada e saÃ­da dos prompts/respostas
- ğŸ“Š Mostra os totais de **Context** (histÃ³rico) e **Eval** (custo de processamento)

---

## ğŸš€ Como usar

1. Instale e configure o [Ollama](https://ollama.ai/) na sua mÃ¡quina.  
2. Clone este repositÃ³rio: 'git clone https://github.com/seuusuario/ChatOllama.git'
3. Abra o projeto no Visual Studio ou VS Code com suporte ao .NET 8.
4. Configure a URL da API do Ollama (se necessÃ¡rio).
5. Compile e execute o projeto.

---
âš ï¸ Aviso Importante 
- ğŸ‘® Nenhum bug de seguranÃ§a foi identificado atÃ© o momento, porÃ©m nÃ£o Ã© recomendado usar em produÃ§Ã£o ou em servidores oficiais.
---
ğŸ“Œ Roadmap (Ideias Futuras)

 - Suporte a histÃ³rico persistente

 - Interface grÃ¡fica mais amigÃ¡vel

 - ConfiguraÃ§Ã£o de parÃ¢metros avanÃ§ados dos modelos

 - Logs detalhados de uso

ğŸ“œ LicenÃ§a

- Este projeto Ã© distribuÃ­do sob a licenÃ§a GNU General Public License (GPL) 2.0
